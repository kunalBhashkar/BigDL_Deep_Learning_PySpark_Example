{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit Card Default Feedforward Network pipeline example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from pyspark.context import SparkContext\n",
    "# from pyspark.sql.session import SparkSession\n",
    "#import pyspark packages\n",
    "## Create SparkContext, SparkSession\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "# Configure MLflow Experiment\n",
    "mlflow_experiment_id = 866112\n",
    "\n",
    "# Including MLflow\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import os\n",
    "print(\"MLflow Version: %s\" % mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/bigdl/util/engine.py:41: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /home/kunal/Downloads/spark-2.4.7-bin-hadoop2.7/, and pyspark is found in: /home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepending /home/kunal/anaconda3/envs/deep_learning/lib/python3.6/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import random as rd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from bigdl.dataset.transformer import *\n",
    "from bigdl.dataset.base import *\n",
    "from bigdl.nn.layer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.util.common import *\n",
    "from utils import *\n",
    "from bigdl.models.ml_pipeline.dl_classifier import *\n",
    "\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml import  Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "sc=SparkContext.getOrCreate(conf=create_spark_conf().setMaster(\"local[4]\").set(\"spark.driver.memory\",\"8g\"))\n",
    "init_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the path for Spark\n",
    "os.environ['JAVA_HOME']=\"/usr/lib/jvm/java-8-oracle\"\n",
    "os.environ['SPARK_HOME'] = \"/home/kunal/Downloads/spark-2.4.7-bin-hadoop2.7\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/kunal/Downloads/jarfiles/bigdl-SPARK_2.2-0.7.0-jar-with-dependencies.jar --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.3,org.apache.spark:spark-streaming-kafka-0-10_2.11:2.4.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # spark = SparkSession(sc)\n",
    "# #Create configuration and sessions\n",
    "# # conf = SparkConf().setAppName(\"Python_Spark_PHD_Complete\").setMaster('local')\n",
    "# sc=SparkContext.getOrCreate(conf=create_spark_conf().setMaster(\"local[4]\"))\n",
    "# # sc = SparkContext(conf=conf)\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Spark Session\n",
    "from pyspark.context import SparkContext\n",
    "# from pyspark.sql.session import SparkSession\n",
    "#import pyspark packages\n",
    "## Create SparkContext, SparkSession\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SQLContext\n",
    "spark = SparkSession(sc)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Parameters\n",
    "learning_rate = 0.1\n",
    "training_epochs = 20\n",
    "batch_size = 1024\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 5\n",
    "n_classes = 2\n",
    "n_hidden_1 = 3 # 1st layer number of features\n",
    "n_hidden_2 = 2 # 1st layer number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read file\n",
    "filename =  \"./data/default-simple.csv\"\n",
    "LABELS = [\"Good\", \"Default\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer 1 (Guess) : 2.5900200641113513\n",
      "Hidden layer 2 (Guess) : 2.23606797749979\n"
     ]
    }
   ],
   "source": [
    "# Number of hidden layers\n",
    "n_hidden_guess = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "print(\"Hidden layer 1 (Guess) : \" + str(n_hidden_guess))\n",
    "\n",
    "n_hidden_guess_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "print(\"Hidden layer 2 (Guess) : \" + str(n_hidden_guess_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Schema \n",
    "schema = StructType() \\\n",
    "              .add(\"id\",IntegerType(),True) \\\n",
    "              .add(\"balance\",IntegerType(),True) \\\n",
    "              .add(\"sex\",IntegerType(),True) \\\n",
    "              .add(\"education\",IntegerType(),True) \\\n",
    "              .add(\"marriage\",IntegerType(),True) \\\n",
    "              .add(\"age\",IntegerType(),True) \\\n",
    "              .add(\"default\",IntegerType(),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "df=pd.read_csv(\"./data/default-simple.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>balance</th>\n",
       "      <th>sex</th>\n",
       "      <th>education</th>\n",
       "      <th>marriage</th>\n",
       "      <th>age</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  balance  sex  education  marriage  age  default\n",
       "0   1    20000    2          2         1   24        2\n",
       "1   2   120000    2          2         2   26        2\n",
       "2   3    90000    2          2         2   34        1\n",
       "3   4    50000    2          2         1   37        1\n",
       "4   5    50000    1          2         1   57        1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Method to convert from pandas dataframe to Spark data frame\n",
    "# Auxiliar functions\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return TimestampType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return FloatType()\n",
    "    else: return StringType()\n",
    "\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "# Given pandas dataframe, it will return a spark's dataframe.\n",
    "def pandas_to_spark(pandas_df):\n",
    "    columns = list(pandas_df.columns)\n",
    "    types = list(pandas_df.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types): \n",
    "          struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlContext.createDataFrame(pandas_df, p_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_training=pandas_to_spark(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+---------+--------+---+-------+\n",
      "| id|balance|sex|education|marriage|age|default|\n",
      "+---+-------+---+---------+--------+---+-------+\n",
      "|  1|  20000|  2|        2|       1| 24|      2|\n",
      "|  2| 120000|  2|        2|       2| 26|      2|\n",
      "|  3|  90000|  2|        2|       2| 34|      1|\n",
      "|  4|  50000|  2|        2|       1| 37|      1|\n",
      "|  5|  50000|  1|        2|       1| 57|      1|\n",
      "|  6|  50000|  1|        1|       2| 37|      1|\n",
      "|  7| 500000|  1|        1|       2| 29|      1|\n",
      "|  8| 100000|  2|        2|       2| 23|      1|\n",
      "|  9| 140000|  2|        3|       1| 28|      1|\n",
      "| 10|  20000|  1|        3|       2| 35|      1|\n",
      "| 11| 200000|  2|        3|       2| 34|      1|\n",
      "| 12| 260000|  2|        1|       2| 51|      1|\n",
      "| 13| 630000|  2|        2|       2| 41|      1|\n",
      "| 14|  70000|  1|        2|       2| 30|      2|\n",
      "| 15| 250000|  1|        1|       2| 29|      1|\n",
      "| 16|  50000|  2|        3|       3| 23|      1|\n",
      "| 17|  20000|  1|        1|       2| 24|      2|\n",
      "| 18| 320000|  1|        1|       1| 49|      1|\n",
      "| 19| 360000|  2|        1|       1| 49|      1|\n",
      "| 20| 180000|  2|        1|       2| 29|      1|\n",
      "+---+-------+---+---------+--------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- sex: long (nullable = true)\n",
      " |-- education: long (nullable = true)\n",
      " |-- marriage: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- default: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- balance: long (nullable = true)\n",
      " |-- sex: long (nullable = true)\n",
      " |-- education: long (nullable = true)\n",
      " |-- marriage: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- default: long (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, column\n",
    "for col_name in cc_training.columns:\n",
    "    cc_training = cc_training.withColumn(col_name, col(col_name).cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_training = cc_training.withColumn('label', cc_training.default.cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- education: integer (nullable = true)\n",
      " |-- marriage: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- default: integer (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|summary|           balance|               sex|         education|          marriage|              age|          default|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "|  count|             30000|             30000|             30000|             30000|            30000|            30000|\n",
      "|   mean|167484.32266666667|1.6037333333333332|1.8531333333333333|1.5518666666666667|          35.4855|           1.2212|\n",
      "| stddev|129747.66156720239|0.4891291960902604|0.7903486597207298|0.5219696006132487|9.217904068090194|0.415061805690933|\n",
      "|    min|             10000|                 1|                 0|                 0|               21|                1|\n",
      "|    max|           1000000|                 2|                 6|                 3|               79|                2|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_training.select('balance','sex','education','marriage','age','default').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#count_classes = pd.value_counts(df['Class'], sort = True)\n",
    "count_classes = pd.value_counts(cc_training.select('default').toPandas()['default'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Status\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into training and validation set\n",
    "(trainingData, validData) = cc_training.select('balance','sex','education','marriage','age','label').randomSplit([.8,.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating pipeline\n",
    "assembler =  VectorAssembler(inputCols=['balance','sex','education','marriage','age'], outputCol=\"assembled\")\n",
    "scaler = StandardScaler(inputCol=\"assembled\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages = [assembler, scaler])\n",
    "pipelineTraining = pipeline.fit(trainingData)\n",
    "cc_data_training = pipelineTraining.transform(trainingData)\n",
    "pipelineTest = pipeline.fit(validData)\n",
    "cc_data_test = pipelineTest.transform(validData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrame in module pyspark.sql.dataframe object:\n",
      "\n",
      "class DataFrame(builtins.object)\n",
      " |  A distributed collection of data grouped into named columns.\n",
      " |  \n",
      " |  A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
      " |  and can be created using various functions in :class:`SparkSession`::\n",
      " |  \n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |  Once created, it can be manipulated using the various domain-specific-language\n",
      " |  (DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
      " |  \n",
      " |  To select a column from the data frame, use the apply method::\n",
      " |  \n",
      " |      ageCol = people.age\n",
      " |  \n",
      " |  A more concrete example::\n",
      " |  \n",
      " |      # To create DataFrame using SparkSession\n",
      " |      people = spark.read.parquet(\"...\")\n",
      " |      department = spark.read.parquet(\"...\")\n",
      " |  \n",
      " |      people.filter(people.age > 30).join(department, people.deptId == department.id) \\\n",
      " |        .groupBy(department.name, \"gender\").agg({\"salary\": \"avg\", \"age\": \"max\"})\n",
      " |  \n",
      " |  .. versionadded:: 1.3\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |      Returns the :class:`Column` denoted by ``name``.\n",
      " |      \n",
      " |      >>> df.select(df.age).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __getitem__(self, item)\n",
      " |      Returns the column as a :class:`Column`.\n",
      " |      \n",
      " |      >>> df.select(df['age']).collect()\n",
      " |      [Row(age=2), Row(age=5)]\n",
      " |      >>> df[ [\"name\", \"age\"]].collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df[ df.age > 3 ].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df[df[0] > 3].collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  __init__(self, jdf, sql_ctx)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  agg(self, *exprs)\n",
      " |      Aggregate on the entire :class:`DataFrame` without groups\n",
      " |      (shorthand for ``df.groupBy.agg()``).\n",
      " |      \n",
      " |      >>> df.agg({\"age\": \"max\"}).collect()\n",
      " |      [Row(max(age)=5)]\n",
      " |      >>> from pyspark.sql import functions as F\n",
      " |      >>> df.agg(F.min(df.age)).collect()\n",
      " |      [Row(min(age)=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  alias(self, alias)\n",
      " |      Returns a new :class:`DataFrame` with an alias set.\n",
      " |      \n",
      " |      :param alias: string, an alias name to be set for the DataFrame.\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df_as1 = df.alias(\"df_as1\")\n",
      " |      >>> df_as2 = df.alias(\"df_as2\")\n",
      " |      >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
      " |      >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\n",
      " |      [Row(name='Bob', name='Bob', age=5), Row(name='Alice', name='Alice', age=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  approxQuantile(self, col, probabilities, relativeError)\n",
      " |      Calculates the approximate quantiles of numerical columns of a\n",
      " |      DataFrame.\n",
      " |      \n",
      " |      The result of this algorithm has the following deterministic bound:\n",
      " |      If the DataFrame has N elements and if we request the quantile at\n",
      " |      probability `p` up to error `err`, then the algorithm will return\n",
      " |      a sample `x` from the DataFrame so that the *exact* rank of `x` is\n",
      " |      close to (p * N). More precisely,\n",
      " |      \n",
      " |        floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\n",
      " |      \n",
      " |      This method implements a variation of the Greenwald-Khanna\n",
      " |      algorithm (with some speed optimizations). The algorithm was first\n",
      " |      present in [[http://dx.doi.org/10.1145/375663.375670\n",
      " |      Space-efficient Online Computation of Quantile Summaries]]\n",
      " |      by Greenwald and Khanna.\n",
      " |      \n",
      " |      Note that null values will be ignored in numerical columns before calculation.\n",
      " |      For columns only containing null values, an empty list is returned.\n",
      " |      \n",
      " |      :param col: str, list.\n",
      " |        Can be a single column name, or a list of names for multiple columns.\n",
      " |      :param probabilities: a list of quantile probabilities\n",
      " |        Each number must belong to [0, 1].\n",
      " |        For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\n",
      " |      :param relativeError:  The relative target precision to achieve\n",
      " |        (>= 0). If set to zero, the exact quantiles are computed, which\n",
      " |        could be very expensive. Note that values greater than 1 are\n",
      " |        accepted but give the same result as 1.\n",
      " |      :return:  the approximate quantiles at the given probabilities. If\n",
      " |        the input `col` is a string, the output is a list of floats. If the\n",
      " |        input `col` is a list or tuple of strings, the output is also a\n",
      " |        list, but each element in it is a list of floats, i.e., the output\n",
      " |        is a list of list of floats.\n",
      " |      \n",
      " |      .. versionchanged:: 2.2\n",
      " |         Added support for multiple columns.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  cache(self)\n",
      " |      Persists the :class:`DataFrame` with the default storage level (C{MEMORY_AND_DISK}).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  checkpoint(self, eager=True)\n",
      " |      Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\n",
      " |      logical plan of this DataFrame, which is especially useful in iterative algorithms where the\n",
      " |      plan may grow exponentially. It will be saved to files inside the checkpoint\n",
      " |      directory set with L{SparkContext.setCheckpointDir()}.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this DataFrame immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  coalesce(self, numPartitions)\n",
      " |      Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\n",
      " |      \n",
      " |      :param numPartitions: int, to specify the target number of partitions\n",
      " |      \n",
      " |      Similar to coalesce defined on an :class:`RDD`, this operation results in a\n",
      " |      narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\n",
      " |      there will not be a shuffle, instead each of the 100 new partitions will\n",
      " |      claim 10 of the current partitions. If a larger number of partitions is requested,\n",
      " |      it will stay at the current number of partitions.\n",
      " |      \n",
      " |      However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\n",
      " |      this may result in your computation taking place on fewer nodes than\n",
      " |      you like (e.g. one node in the case of numPartitions = 1). To avoid this,\n",
      " |      you can call repartition(). This will add a shuffle step, but means the\n",
      " |      current upstream partitions will be executed in parallel (per whatever\n",
      " |      the current partitioning is).\n",
      " |      \n",
      " |      >>> df.coalesce(1).rdd.getNumPartitions()\n",
      " |      1\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  colRegex(self, colName)\n",
      " |      Selects column based on the column name specified as a regex and returns it\n",
      " |      as :class:`Column`.\n",
      " |      \n",
      " |      :param colName: string, column name specified as a regex.\n",
      " |      \n",
      " |      >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\n",
      " |      >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\n",
      " |      +----+\n",
      " |      |Col2|\n",
      " |      +----+\n",
      " |      |   1|\n",
      " |      |   2|\n",
      " |      |   3|\n",
      " |      +----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  collect(self)\n",
      " |      Returns all the records as a list of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  corr(self, col1, col2, method=None)\n",
      " |      Calculates the correlation of two columns of a DataFrame as a double value.\n",
      " |      Currently only supports the Pearson Correlation Coefficient.\n",
      " |      :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      :param method: The correlation method. Currently only supports \"pearson\"\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  count(self)\n",
      " |      Returns the number of rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  cov(self, col1, col2)\n",
      " |      Calculate the sample covariance for the given columns, specified by their names, as a\n",
      " |      double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column\n",
      " |      :param col2: The name of the second column\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  createGlobalTempView(self, name)\n",
      " |      Creates a global temporary view with this DataFrame.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createGlobalTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createGlobalTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  createOrReplaceGlobalTempView(self, name)\n",
      " |      Creates or replaces a global temporary view using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary view is tied to this Spark application.\n",
      " |      \n",
      " |      >>> df.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceGlobalTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from global_temp.people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropGlobalTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  createOrReplaceTempView(self, name)\n",
      " |      Creates or replaces a local temporary view with this DataFrame.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"people\")\n",
      " |      >>> df2 = df.filter(df.age > 3)\n",
      " |      >>> df2.createOrReplaceTempView(\"people\")\n",
      " |      >>> df3 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df3.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  createTempView(self, name)\n",
      " |      Creates a local temporary view with this DataFrame.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      throws :class:`TempTableAlreadyExistsException`, if the view name already exists in the\n",
      " |      catalog.\n",
      " |      \n",
      " |      >>> df.createTempView(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> df.createTempView(\"people\")  # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |      ...\n",
      " |      AnalysisException: u\"Temporary table 'people' already exists;\"\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  crossJoin(self, other)\n",
      " |      Returns the cartesian product with another :class:`DataFrame`.\n",
      " |      \n",
      " |      :param other: Right side of the cartesian product.\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df2.select(\"name\", \"height\").collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85)]\n",
      " |      >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\n",
      " |      [Row(age=2, name='Alice', height=80), Row(age=2, name='Alice', height=85),\n",
      " |       Row(age=5, name='Bob', height=80), Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  crosstab(self, col1, col2)\n",
      " |      Computes a pair-wise frequency table of the given columns. Also known as a contingency\n",
      " |      table. The number of distinct values for each column should be less than 1e4. At most 1e6\n",
      " |      non-zero pair frequencies will be returned.\n",
      " |      The first column of each row will be the distinct values of `col1` and the column names\n",
      " |      will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n",
      " |      Pairs that have no occurrences will have zero as their counts.\n",
      " |      :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n",
      " |      \n",
      " |      :param col1: The name of the first column. Distinct items will make the first item of\n",
      " |          each row.\n",
      " |      :param col2: The name of the second column. Distinct items will make the column names\n",
      " |          of the DataFrame.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  cube(self, *cols)\n",
      " |      Create a multi-dimensional cube for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      >>> df.cube(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      | null|   2|    1|\n",
      " |      | null|   5|    1|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  describe(self, *cols)\n",
      " |      Computes basic statistics for numeric and string columns.\n",
      " |      \n",
      " |      This include count, mean, stddev, min, and max. If no columns are\n",
      " |      given, this function computes statistics for all numerical or string columns.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting DataFrame.\n",
      " |      \n",
      " |      >>> df.describe(['age']).show()\n",
      " |      +-------+------------------+\n",
      " |      |summary|               age|\n",
      " |      +-------+------------------+\n",
      " |      |  count|                 2|\n",
      " |      |   mean|               3.5|\n",
      " |      | stddev|2.1213203435596424|\n",
      " |      |    min|                 2|\n",
      " |      |    max|                 5|\n",
      " |      +-------+------------------+\n",
      " |      >>> df.describe().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      Use summary for expanded statistics and control over which statistics to compute.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  distinct(self)\n",
      " |      Returns a new :class:`DataFrame` containing the distinct rows in this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.distinct().count()\n",
      " |      2\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  drop(self, *cols)\n",
      " |      Returns a new :class:`DataFrame` that drops the specified column.\n",
      " |      This is a no-op if schema doesn't contain the given column name(s).\n",
      " |      \n",
      " |      :param cols: a string name of the column to drop, or a\n",
      " |          :class:`Column` to drop, or a list of string name of the columns to drop.\n",
      " |      \n",
      " |      >>> df.drop('age').collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.drop(df.age).collect()\n",
      " |      [Row(name='Alice'), Row(name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\n",
      " |      [Row(age=5, height=85, name='Bob')]\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\n",
      " |      [Row(age=5, name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\n",
      " |      [Row(name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropDuplicates(self, subset=None)\n",
      " |      Return a new :class:`DataFrame` with duplicate rows removed,\n",
      " |      optionally only considering certain columns.\n",
      " |      \n",
      " |      For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\n",
      " |      :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\n",
      " |      duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\n",
      " |      be and system will accordingly limit the state. In addition, too late data older than\n",
      " |      watermark will be dropped to avoid any possibility of duplicates.\n",
      " |      \n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> df = sc.parallelize([ \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=5, height=80), \\\n",
      " |      ...     Row(name='Alice', age=10, height=80)]).toDF()\n",
      " |      >>> df.dropDuplicates().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df.dropDuplicates(['name', 'height']).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      |  5|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  drop_duplicates = dropDuplicates(self, subset=None)\n",
      " |      :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  dropna(self, how='any', thresh=None, subset=None)\n",
      " |      Returns a new :class:`DataFrame` omitting rows with null values.\n",
      " |      :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n",
      " |      \n",
      " |      :param how: 'any' or 'all'.\n",
      " |          If 'any', drop a row if it contains any nulls.\n",
      " |          If 'all', drop a row only if all its values are null.\n",
      " |      :param thresh: int, default None\n",
      " |          If specified, drop rows that have less than `thresh` non-null values.\n",
      " |          This overwrites the `how` parameter.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |      \n",
      " |      >>> df4.na.drop().show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  exceptAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\n",
      " |      not in another :class:`DataFrame` while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT ALL` in SQL.\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame(\n",
      " |      ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.exceptAll(df2).show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  a|  2|\n",
      " |      |  c|  4|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  explain(self, extended=False)\n",
      " |      Prints the (logical and physical) plans to the console for debugging purpose.\n",
      " |      \n",
      " |      :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\n",
      " |      \n",
      " |      >>> df.explain()\n",
      " |      == Physical Plan ==\n",
      " |      Scan ExistingRDD[age#0,name#1]\n",
      " |      \n",
      " |      >>> df.explain(True)\n",
      " |      == Parsed Logical Plan ==\n",
      " |      ...\n",
      " |      == Analyzed Logical Plan ==\n",
      " |      ...\n",
      " |      == Optimized Logical Plan ==\n",
      " |      ...\n",
      " |      == Physical Plan ==\n",
      " |      ...\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  fillna(self, value, subset=None)\n",
      " |      Replace null values, alias for ``na.fill()``.\n",
      " |      :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\n",
      " |      \n",
      " |      :param value: int, long, float, string, bool or dict.\n",
      " |          Value to replace null values with.\n",
      " |          If the value is a dict, then `subset` is ignored and `value` must be a mapping\n",
      " |          from column name (string) to replacement value. The replacement value must be\n",
      " |          an int, long, float, boolean, or string.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.fill(50).show()\n",
      " |      +---+------+-----+\n",
      " |      |age|height| name|\n",
      " |      +---+------+-----+\n",
      " |      | 10|    80|Alice|\n",
      " |      |  5|    50|  Bob|\n",
      " |      | 50|    50|  Tom|\n",
      " |      | 50|    50| null|\n",
      " |      +---+------+-----+\n",
      " |      \n",
      " |      >>> df5.na.fill(False).show()\n",
      " |      +----+-------+-----+\n",
      " |      | age|   name|  spy|\n",
      " |      +----+-------+-----+\n",
      " |      |  10|  Alice|false|\n",
      " |      |   5|    Bob|false|\n",
      " |      |null|Mallory| true|\n",
      " |      +----+-------+-----+\n",
      " |      \n",
      " |      >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\n",
      " |      +---+------+-------+\n",
      " |      |age|height|   name|\n",
      " |      +---+------+-------+\n",
      " |      | 10|    80|  Alice|\n",
      " |      |  5|  null|    Bob|\n",
      " |      | 50|  null|    Tom|\n",
      " |      | 50|  null|unknown|\n",
      " |      +---+------+-------+\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  filter(self, condition)\n",
      " |      Filters rows using the given condition.\n",
      " |      \n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      :param condition: a :class:`Column` of :class:`types.BooleanType`\n",
      " |          or a string of SQL expression.\n",
      " |      \n",
      " |      >>> df.filter(df.age > 3).collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(df.age == 2).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      >>> df.filter(\"age > 3\").collect()\n",
      " |      [Row(age=5, name='Bob')]\n",
      " |      >>> df.where(\"age = 2\").collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  first(self)\n",
      " |      Returns the first row as a :class:`Row`.\n",
      " |      \n",
      " |      >>> df.first()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreach(self, f)\n",
      " |      Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a shorthand for ``df.rdd.foreach()``.\n",
      " |      \n",
      " |      >>> def f(person):\n",
      " |      ...     print(person.name)\n",
      " |      >>> df.foreach(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  foreachPartition(self, f)\n",
      " |      Applies the ``f`` function to each partition of this :class:`DataFrame`.\n",
      " |      \n",
      " |      This a shorthand for ``df.rdd.foreachPartition()``.\n",
      " |      \n",
      " |      >>> def f(people):\n",
      " |      ...     for person in people:\n",
      " |      ...         print(person.name)\n",
      " |      >>> df.foreachPartition(f)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  freqItems(self, cols, support=None)\n",
      " |      Finding frequent items for columns, possibly with false positives. Using the\n",
      " |      frequent element count algorithm described in\n",
      " |      \"http://dx.doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\n",
      " |      :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting DataFrame.\n",
      " |      \n",
      " |      :param cols: Names of the columns to calculate frequent items for as a list or tuple of\n",
      " |          strings.\n",
      " |      :param support: The frequency with which to consider an item 'frequent'. Default is 1%.\n",
      " |          The support must be greater than 1e-4.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  groupBy(self, *cols)\n",
      " |      Groups the :class:`DataFrame` using the specified columns,\n",
      " |      so we can run aggregation on them. See :class:`GroupedData`\n",
      " |      for all the available aggregate functions.\n",
      " |      \n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      :param cols: list of columns to group by.\n",
      " |          Each element should be a column name (string) or an expression (:class:`Column`).\n",
      " |      \n",
      " |      >>> df.groupBy().avg().collect()\n",
      " |      [Row(avg(age)=3.5)]\n",
      " |      >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(df.name).avg().collect())\n",
      " |      [Row(name='Alice', avg(age)=2.0), Row(name='Bob', avg(age)=5.0)]\n",
      " |      >>> sorted(df.groupBy(['name', df.age]).count().collect())\n",
      " |      [Row(name='Alice', age=2, count=1), Row(name='Bob', age=5, count=1)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  groupby = groupBy(self, *cols)\n",
      " |      :func:`groupby` is an alias for :func:`groupBy`.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  head(self, n=None)\n",
      " |      Returns the first ``n`` rows.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting array is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      :param n: int, default 1. Number of rows to return.\n",
      " |      :return: If n is greater than 1, return a list of :class:`Row`.\n",
      " |          If n is 1, return a single Row.\n",
      " |      \n",
      " |      >>> df.head()\n",
      " |      Row(age=2, name='Alice')\n",
      " |      >>> df.head(1)\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  hint(self, name, *parameters)\n",
      " |      Specifies some hint on the current DataFrame.\n",
      " |      \n",
      " |      :param name: A name of the hint.\n",
      " |      :param parameters: Optional parameters.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\n",
      " |      +----+---+------+\n",
      " |      |name|age|height|\n",
      " |      +----+---+------+\n",
      " |      | Bob|  5|    85|\n",
      " |      +----+---+------+\n",
      " |      \n",
      " |      .. versionadded:: 2.2\n",
      " |  \n",
      " |  intersect(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows only in\n",
      " |      both this frame and another frame.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  intersectAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in both this dataframe and other\n",
      " |      dataframe while preserving duplicates.\n",
      " |      \n",
      " |      This is equivalent to `INTERSECT ALL` in SQL.\n",
      " |      >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
      " |      >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
      " |      \n",
      " |      >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\n",
      " |      +---+---+\n",
      " |      | C1| C2|\n",
      " |      +---+---+\n",
      " |      |  a|  1|\n",
      " |      |  a|  1|\n",
      " |      |  b|  3|\n",
      " |      +---+---+\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.4\n",
      " |  \n",
      " |  isLocal(self)\n",
      " |      Returns ``True`` if the :func:`collect` and :func:`take` methods can be run locally\n",
      " |      (without any Spark executors).\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  join(self, other, on=None, how=None)\n",
      " |      Joins with another :class:`DataFrame`, using the given join expression.\n",
      " |      \n",
      " |      :param other: Right side of the join\n",
      " |      :param on: a string for the join column name, a list of column names,\n",
      " |          a join expression (Column), or a list of Columns.\n",
      " |          If `on` is a string or a list of strings indicating the name of the join column(s),\n",
      " |          the column(s) must exist on both sides, and this performs an equi-join.\n",
      " |      :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\n",
      " |          ``full``, ``full_outer``, ``left``, ``left_outer``, ``right``, ``right_outer``,\n",
      " |          ``left_semi``, and ``left_anti``.\n",
      " |      \n",
      " |      The following performs a full outer join between ``df1`` and ``df2``.\n",
      " |      \n",
      " |      >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()\n",
      " |      [Row(name=None, height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()\n",
      " |      [Row(name='Tom', height=80), Row(name='Bob', height=85), Row(name='Alice', height=None)]\n",
      " |      \n",
      " |      >>> cond = [df.name == df3.name, df.age == df3.age]\n",
      " |      >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      >>> df.join(df2, 'name').select(df.name, df2.height).collect()\n",
      " |      [Row(name='Bob', height=85)]\n",
      " |      \n",
      " |      >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\n",
      " |      [Row(name='Bob', age=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  limit(self, num)\n",
      " |      Limits the result count to the number specified.\n",
      " |      \n",
      " |      >>> df.limit(1).collect()\n",
      " |      [Row(age=2, name='Alice')]\n",
      " |      >>> df.limit(0).collect()\n",
      " |      []\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  localCheckpoint(self, eager=True)\n",
      " |      Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\n",
      " |      truncate the logical plan of this DataFrame, which is especially useful in iterative\n",
      " |      algorithms where the plan may grow exponentially. Local checkpoints are stored in the\n",
      " |      executors using the caching subsystem and therefore they are not reliable.\n",
      " |      \n",
      " |      :param eager: Whether to checkpoint this DataFrame immediately\n",
      " |      \n",
      " |      .. note:: Experimental\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  orderBy = sort(self, *cols, **kwargs)\n",
      " |  \n",
      " |  persist(self, storageLevel=StorageLevel(True, True, False, False, 1))\n",
      " |      Sets the storage level to persist the contents of the :class:`DataFrame` across\n",
      " |      operations after the first time it is computed. This can only be used to assign\n",
      " |      a new storage level if the :class:`DataFrame` does not have a storage level set yet.\n",
      " |      If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\n",
      " |      \n",
      " |      .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  printSchema(self)\n",
      " |      Prints out the schema in the tree format.\n",
      " |      \n",
      " |      >>> df.printSchema()\n",
      " |      root\n",
      " |       |-- age: integer (nullable = true)\n",
      " |       |-- name: string (nullable = true)\n",
      " |      <BLANKLINE>\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  randomSplit(self, weights, seed=None)\n",
      " |      Randomly splits this :class:`DataFrame` with the provided weights.\n",
      " |      \n",
      " |      :param weights: list of doubles as weights with which to split the DataFrame. Weights will\n",
      " |          be normalized if they don't sum up to 1.0.\n",
      " |      :param seed: The seed for sampling.\n",
      " |      \n",
      " |      >>> splits = df4.randomSplit([1.0, 2.0], 24)\n",
      " |      >>> splits[0].count()\n",
      " |      1\n",
      " |      \n",
      " |      >>> splits[1].count()\n",
      " |      3\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerTempTable(self, name)\n",
      " |      Registers this DataFrame as a temporary table using the given name.\n",
      " |      \n",
      " |      The lifetime of this temporary table is tied to the :class:`SparkSession`\n",
      " |      that was used to create this :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df.registerTempTable(\"people\")\n",
      " |      >>> df2 = spark.sql(\"select * from people\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      >>> spark.catalog.dropTempView(\"people\")\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartition(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting DataFrame is hash partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      .. versionchanged:: 1.6\n",
      " |         Added optional arguments to specify the partitioning columns. Also made numPartitions\n",
      " |         optional if partitioning columns are specified.\n",
      " |      \n",
      " |      >>> df.repartition(10).rdd.getNumPartitions()\n",
      " |      10\n",
      " |      >>> data = df.union(df).repartition(\"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      >>> data = data.repartition(7, \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> data.rdd.getNumPartitions()\n",
      " |      7\n",
      " |      >>> data = data.repartition(\"name\", \"age\")\n",
      " |      >>> data.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  5|  Bob|\n",
      " |      |  5|  Bob|\n",
      " |      |  2|Alice|\n",
      " |      |  2|Alice|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  repartitionByRange(self, numPartitions, *cols)\n",
      " |      Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\n",
      " |      resulting DataFrame is range partitioned.\n",
      " |      \n",
      " |      :param numPartitions:\n",
      " |          can be an int to specify the target number of partitions or a Column.\n",
      " |          If it is a Column, it will be used as the first partitioning column. If not specified,\n",
      " |          the default number of partitions is used.\n",
      " |      \n",
      " |      At least one partition-by expression must be specified.\n",
      " |      When no explicit sort order is specified, \"ascending nulls first\" is assumed.\n",
      " |      \n",
      " |      >>> df.repartitionByRange(2, \"age\").rdd.getNumPartitions()\n",
      " |      2\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.repartitionByRange(1, \"age\").rdd.getNumPartitions()\n",
      " |      1\n",
      " |      >>> data = df.repartitionByRange(\"age\")\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 2.4.0\n",
      " |  \n",
      " |  replace(self, to_replace, value=<no value>, subset=None)\n",
      " |      Returns a new :class:`DataFrame` replacing a value with another value.\n",
      " |      :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\n",
      " |      aliases of each other.\n",
      " |      Values to_replace and value must have the same type and can only be numerics, booleans,\n",
      " |      or strings. Value can have None. When replacing, the new value will be cast\n",
      " |      to the type of the existing column.\n",
      " |      For numeric replacements all values to be replaced should have unique\n",
      " |      floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\n",
      " |      and arbitrary replacement will be used.\n",
      " |      \n",
      " |      :param to_replace: bool, int, long, float, string, list or dict.\n",
      " |          Value to be replaced.\n",
      " |          If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\n",
      " |          must be a mapping between a value and a replacement.\n",
      " |      :param value: bool, int, long, float, string, list or None.\n",
      " |          The replacement value must be a bool, int, long, float, string or None. If `value` is a\n",
      " |          list, `value` should be of the same length and type as `to_replace`.\n",
      " |          If `value` is a scalar and `to_replace` is a sequence, then `value` is\n",
      " |          used as a replacement for each item in `to_replace`.\n",
      " |      :param subset: optional list of column names to consider.\n",
      " |          Columns specified in subset that do not have matching data type are ignored.\n",
      " |          For example, if `value` is a string, and subset contains a non-string column,\n",
      " |          then the non-string column is simply ignored.\n",
      " |      \n",
      " |      >>> df4.na.replace(10, 20).show()\n",
      " |      +----+------+-----+\n",
      " |      | age|height| name|\n",
      " |      +----+------+-----+\n",
      " |      |  20|    80|Alice|\n",
      " |      |   5|  null|  Bob|\n",
      " |      |null|  null|  Tom|\n",
      " |      |null|  null| null|\n",
      " |      +----+------+-----+\n",
      " |      \n",
      " |      >>> df4.na.replace('Alice', None).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace({'Alice': None}).show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|null|\n",
      " |      |   5|  null| Bob|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\n",
      " |      +----+------+----+\n",
      " |      | age|height|name|\n",
      " |      +----+------+----+\n",
      " |      |  10|    80|   A|\n",
      " |      |   5|  null|   B|\n",
      " |      |null|  null| Tom|\n",
      " |      |null|  null|null|\n",
      " |      +----+------+----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  rollup(self, *cols)\n",
      " |      Create a multi-dimensional rollup for the current :class:`DataFrame` using\n",
      " |      the specified columns, so we can run aggregation on them.\n",
      " |      \n",
      " |      >>> df.rollup(\"name\", df.age).count().orderBy(\"name\", \"age\").show()\n",
      " |      +-----+----+-----+\n",
      " |      | name| age|count|\n",
      " |      +-----+----+-----+\n",
      " |      | null|null|    2|\n",
      " |      |Alice|null|    1|\n",
      " |      |Alice|   2|    1|\n",
      " |      |  Bob|null|    1|\n",
      " |      |  Bob|   5|    1|\n",
      " |      +-----+----+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  sample(self, withReplacement=None, fraction=None, seed=None)\n",
      " |      Returns a sampled subset of this :class:`DataFrame`.\n",
      " |      \n",
      " |      :param withReplacement: Sample with replacement or not (default False).\n",
      " |      :param fraction: Fraction of rows to generate, range [0.0, 1.0].\n",
      " |      :param seed: Seed for sampling (default a random seed).\n",
      " |      \n",
      " |      .. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
      " |          count of the given :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\n",
      " |      \n",
      " |      >>> df = spark.range(10)\n",
      " |      >>> df.sample(0.5, 3).count()\n",
      " |      4\n",
      " |      >>> df.sample(fraction=0.5, seed=3).count()\n",
      " |      4\n",
      " |      >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\n",
      " |      1\n",
      " |      >>> df.sample(1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(fraction=1.0).count()\n",
      " |      10\n",
      " |      >>> df.sample(False, fraction=1.0).count()\n",
      " |      10\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sampleBy(self, col, fractions, seed=None)\n",
      " |      Returns a stratified sample without replacement based on the\n",
      " |      fraction given on each stratum.\n",
      " |      \n",
      " |      :param col: column that defines strata\n",
      " |      :param fractions:\n",
      " |          sampling fraction for each stratum. If a stratum is not\n",
      " |          specified, we treat its fraction as zero.\n",
      " |      :param seed: random seed\n",
      " |      :return: a new DataFrame that represents the stratified sample\n",
      " |      \n",
      " |      >>> from pyspark.sql.functions import col\n",
      " |      >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
      " |      >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
      " |      >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\n",
      " |      +---+-----+\n",
      " |      |key|count|\n",
      " |      +---+-----+\n",
      " |      |  0|    5|\n",
      " |      |  1|    9|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  select(self, *cols)\n",
      " |      Projects a set of expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      :param cols: list of column names (string) or expressions (:class:`Column`).\n",
      " |          If one of the column names is '*', that column is expanded to include all columns\n",
      " |          in the current DataFrame.\n",
      " |      \n",
      " |      >>> df.select('*').collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.select('name', 'age').collect()\n",
      " |      [Row(name='Alice', age=2), Row(name='Bob', age=5)]\n",
      " |      >>> df.select(df.name, (df.age + 10).alias('age')).collect()\n",
      " |      [Row(name='Alice', age=12), Row(name='Bob', age=15)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  selectExpr(self, *expr)\n",
      " |      Projects a set of SQL expressions and returns a new :class:`DataFrame`.\n",
      " |      \n",
      " |      This is a variant of :func:`select` that accepts SQL expressions.\n",
      " |      \n",
      " |      >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\n",
      " |      [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  show(self, n=20, truncate=True, vertical=False)\n",
      " |      Prints the first ``n`` rows to the console.\n",
      " |      \n",
      " |      :param n: Number of rows to show.\n",
      " |      :param truncate: If set to True, truncate strings longer than 20 chars by default.\n",
      " |          If set to a number greater than one, truncates long strings to length ``truncate``\n",
      " |          and align cells right.\n",
      " |      :param vertical: If set to True, print output rows vertically (one line\n",
      " |          per column value).\n",
      " |      \n",
      " |      >>> df\n",
      " |      DataFrame[age: int, name: string]\n",
      " |      >>> df.show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      >>> df.show(truncate=3)\n",
      " |      +---+----+\n",
      " |      |age|name|\n",
      " |      +---+----+\n",
      " |      |  2| Ali|\n",
      " |      |  5| Bob|\n",
      " |      +---+----+\n",
      " |      >>> df.show(vertical=True)\n",
      " |      -RECORD 0-----\n",
      " |       age  | 2\n",
      " |       name | Alice\n",
      " |      -RECORD 1-----\n",
      " |       age  | 5\n",
      " |       name | Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sort(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default True).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sort(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.sort(\"age\", ascending=False).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy(df.age.desc()).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> from pyspark.sql.functions import *\n",
      " |      >>> df.sort(asc(\"age\")).collect()\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      >>> df.orderBy(desc(\"age\"), \"name\").collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      >>> df.orderBy([\"age\", \"name\"], ascending=[0, 1]).collect()\n",
      " |      [Row(age=5, name='Bob'), Row(age=2, name='Alice')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sortWithinPartitions(self, *cols, **kwargs)\n",
      " |      Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\n",
      " |      \n",
      " |      :param cols: list of :class:`Column` or column names to sort by.\n",
      " |      :param ascending: boolean or list of boolean (default True).\n",
      " |          Sort ascending vs. descending. Specify list for multiple sort orders.\n",
      " |          If a list is specified, length of the list must equal length of the `cols`.\n",
      " |      \n",
      " |      >>> df.sortWithinPartitions(\"age\", ascending=False).show()\n",
      " |      +---+-----+\n",
      " |      |age| name|\n",
      " |      +---+-----+\n",
      " |      |  2|Alice|\n",
      " |      |  5|  Bob|\n",
      " |      +---+-----+\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  subtract(self, other)\n",
      " |      Return a new :class:`DataFrame` containing rows in this frame\n",
      " |      but not in another frame.\n",
      " |      \n",
      " |      This is equivalent to `EXCEPT DISTINCT` in SQL.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  summary(self, *statistics)\n",
      " |      Computes specified statistics for numeric and string columns. Available statistics are:\n",
      " |      - count\n",
      " |      - mean\n",
      " |      - stddev\n",
      " |      - min\n",
      " |      - max\n",
      " |      - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
      " |      \n",
      " |      If no statistics are given, this function computes count, mean, stddev, min,\n",
      " |      approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\n",
      " |      \n",
      " |      .. note:: This function is meant for exploratory data analysis, as we make no\n",
      " |          guarantee about the backward compatibility of the schema of the resulting DataFrame.\n",
      " |      \n",
      " |      >>> df.summary().show()\n",
      " |      +-------+------------------+-----+\n",
      " |      |summary|               age| name|\n",
      " |      +-------+------------------+-----+\n",
      " |      |  count|                 2|    2|\n",
      " |      |   mean|               3.5| null|\n",
      " |      | stddev|2.1213203435596424| null|\n",
      " |      |    min|                 2|Alice|\n",
      " |      |    25%|                 2| null|\n",
      " |      |    50%|                 2| null|\n",
      " |      |    75%|                 5| null|\n",
      " |      |    max|                 5|  Bob|\n",
      " |      +-------+------------------+-----+\n",
      " |      \n",
      " |      >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\n",
      " |      +-------+---+-----+\n",
      " |      |summary|age| name|\n",
      " |      +-------+---+-----+\n",
      " |      |  count|  2|    2|\n",
      " |      |    min|  2|Alice|\n",
      " |      |    25%|  2| null|\n",
      " |      |    75%|  5| null|\n",
      " |      |    max|  5|  Bob|\n",
      " |      +-------+---+-----+\n",
      " |      \n",
      " |      To do a summary for specific columns first select them:\n",
      " |      \n",
      " |      >>> df.select(\"age\", \"name\").summary(\"count\").show()\n",
      " |      +-------+---+----+\n",
      " |      |summary|age|name|\n",
      " |      +-------+---+----+\n",
      " |      |  count|  2|   2|\n",
      " |      +-------+---+----+\n",
      " |      \n",
      " |      See also describe for basic statistics.\n",
      " |      \n",
      " |      .. versionadded:: 2.3.0\n",
      " |  \n",
      " |  take(self, num)\n",
      " |      Returns the first ``num`` rows as a :class:`list` of :class:`Row`.\n",
      " |      \n",
      " |      >>> df.take(2)\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toDF(self, *cols)\n",
      " |      Returns a new class:`DataFrame` that with new specified column names\n",
      " |      \n",
      " |      :param cols: list of new column names (string)\n",
      " |      \n",
      " |      >>> df.toDF('f1', 'f2').collect()\n",
      " |      [Row(f1=2, f2='Alice'), Row(f1=5, f2='Bob')]\n",
      " |  \n",
      " |  toJSON(self, use_unicode=True)\n",
      " |      Converts a :class:`DataFrame` into a :class:`RDD` of string.\n",
      " |      \n",
      " |      Each row is turned into a JSON document as one element in the returned RDD.\n",
      " |      \n",
      " |      >>> df.toJSON().first()\n",
      " |      '{\"age\":2,\"name\":\"Alice\"}'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  toLocalIterator(self)\n",
      " |      Returns an iterator that contains all of the rows in this :class:`DataFrame`.\n",
      " |      The iterator will consume as much memory as the largest partition in this DataFrame.\n",
      " |      \n",
      " |      >>> list(df.toLocalIterator())\n",
      " |      [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  toPandas(self)\n",
      " |      Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\n",
      " |      \n",
      " |      This is only available if Pandas is installed and available.\n",
      " |      \n",
      " |      .. note:: This method should only be used if the resulting Pandas's DataFrame is expected\n",
      " |          to be small, as all the data is loaded into the driver's memory.\n",
      " |      \n",
      " |      .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n",
      " |      \n",
      " |      >>> df.toPandas()  # doctest: +SKIP\n",
      " |         age   name\n",
      " |      0    2  Alice\n",
      " |      1    5    Bob\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  union(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another frame.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  unionAll(self, other)\n",
      " |      Return a new :class:`DataFrame` containing union of rows in this and another frame.\n",
      " |      \n",
      " |      This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\n",
      " |      (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      Also as standard in SQL, this function resolves columns by position (not by name).\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.0, use :func:`union` instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  unionByName(self, other)\n",
      " |      Returns a new :class:`DataFrame` containing union of rows in this and another frame.\n",
      " |      \n",
      " |      This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\n",
      " |      union (that does deduplication of elements), use this function followed by :func:`distinct`.\n",
      " |      \n",
      " |      The difference between this function and :func:`union` is that this function\n",
      " |      resolves columns by name (not by position):\n",
      " |      \n",
      " |      >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
      " |      >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
      " |      >>> df1.unionByName(df2).show()\n",
      " |      +----+----+----+\n",
      " |      |col0|col1|col2|\n",
      " |      +----+----+----+\n",
      " |      |   1|   2|   3|\n",
      " |      |   6|   4|   5|\n",
      " |      +----+----+----+\n",
      " |      \n",
      " |      .. versionadded:: 2.3\n",
      " |  \n",
      " |  unpersist(self, blocking=False)\n",
      " |      Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n",
      " |      memory and disk.\n",
      " |      \n",
      " |      .. note:: `blocking` default has changed to False to match Scala in 2.0.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  where = filter(self, condition)\n",
      " |      :func:`where` is an alias for :func:`filter`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumn(self, colName, col)\n",
      " |      Returns a new :class:`DataFrame` by adding a column or replacing the\n",
      " |      existing column that has the same name.\n",
      " |      \n",
      " |      The column expression must be an expression over this DataFrame; attempting to add\n",
      " |      a column from some other dataframe will raise an error.\n",
      " |      \n",
      " |      :param colName: string, name of the new column.\n",
      " |      :param col: a :class:`Column` expression for the new column.\n",
      " |      \n",
      " |      >>> df.withColumn('age2', df.age + 2).collect()\n",
      " |      [Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withColumnRenamed(self, existing, new)\n",
      " |      Returns a new :class:`DataFrame` by renaming an existing column.\n",
      " |      This is a no-op if schema doesn't contain the given column name.\n",
      " |      \n",
      " |      :param existing: string, name of the existing column to rename.\n",
      " |      :param new: string, new name of the column.\n",
      " |      \n",
      " |      >>> df.withColumnRenamed('age', 'age2').collect()\n",
      " |      [Row(age2=2, name='Alice'), Row(age2=5, name='Bob')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  withWatermark(self, eventTime, delayThreshold)\n",
      " |      Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\n",
      " |      in time before which we assume no more late data is going to arrive.\n",
      " |      \n",
      " |      Spark will use this watermark for several purposes:\n",
      " |        - To know when a given time window aggregation can be finalized and thus can be emitted\n",
      " |          when using output modes that do not allow updates.\n",
      " |      \n",
      " |        - To minimize the amount of state that we need to keep for on-going aggregations.\n",
      " |      \n",
      " |      The current watermark is computed by looking at the `MAX(eventTime)` seen across\n",
      " |      all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\n",
      " |      of coordinating this value across partitions, the actual watermark used is only guaranteed\n",
      " |      to be at least `delayThreshold` behind the actual event time.  In some cases we may still\n",
      " |      process records that arrive more than `delayThreshold` late.\n",
      " |      \n",
      " |      :param eventTime: the name of the column that contains the event time of the row.\n",
      " |      :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\n",
      " |          latest record that has been processed in the form of an interval\n",
      " |          (e.g. \"1 minute\" or \"5 hours\").\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      >>> sdf.select('name', sdf.time.cast('timestamp')).withWatermark('time', '10 minutes')\n",
      " |      DataFrame[name: string, time: timestamp]\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  columns\n",
      " |      Returns all column names as a list.\n",
      " |      \n",
      " |      >>> df.columns\n",
      " |      ['age', 'name']\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dtypes\n",
      " |      Returns all column names and their data types as a list.\n",
      " |      \n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'int'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  isStreaming\n",
      " |      Returns true if this :class:`Dataset` contains one or more sources that continuously\n",
      " |      return data as it arrives. A :class:`Dataset` that reads data from a streaming source\n",
      " |      must be executed as a :class:`StreamingQuery` using the :func:`start` method in\n",
      " |      :class:`DataStreamWriter`.  Methods that return a single answer, (e.g., :func:`count` or\n",
      " |      :func:`collect`) will throw an :class:`AnalysisException` when there is a streaming\n",
      " |      source present.\n",
      " |      \n",
      " |      .. note:: Evolving\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  na\n",
      " |      Returns a :class:`DataFrameNaFunctions` for handling missing values.\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      " |  \n",
      " |  rdd\n",
      " |      Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  schema\n",
      " |      Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\n",
      " |      \n",
      " |      >>> df.schema\n",
      " |      StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  stat\n",
      " |      Returns a :class:`DataFrameStatFunctions` for statistic functions.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  storageLevel\n",
      " |      Get the :class:`DataFrame`'s current storage level.\n",
      " |      \n",
      " |      >>> df.storageLevel\n",
      " |      StorageLevel(False, False, False, False, 1)\n",
      " |      >>> df.cache().storageLevel\n",
      " |      StorageLevel(True, True, False, True, 1)\n",
      " |      >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\n",
      " |      StorageLevel(True, False, False, False, 2)\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  write\n",
      " |      Interface for saving the content of the non-streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      :return: :class:`DataFrameWriter`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  writeStream\n",
      " |      Interface for saving the content of the streaming :class:`DataFrame` out into external\n",
      " |      storage.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamWriter`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cc_data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+--------+---+-----+--------------------+--------------------+\n",
      "|balance|sex|education|marriage|age|label|           assembled|            features|\n",
      "+-------+---+---------+--------+---+-----+--------------------+--------------------+\n",
      "|  10000|  1|        1|       1| 41|  1.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 22|  2.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 22|  2.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 23|  1.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 23|  2.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 24|  2.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 24|  2.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 29|  1.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 30|  1.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        1|       2| 33|  1.0|[10000.0,1.0,1.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       1| 32|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       1| 42|  2.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       1| 45|  2.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       1| 56|  2.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       1| 59|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       1| 62|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       2| 21|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       2| 21|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       2| 22|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "|  10000|  1|        2|       2| 22|  1.0|[10000.0,1.0,2.0,...|[0.07704545366067...|\n",
      "+-------+---+---------+--------+---+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top 20 rows\n",
    "cc_data_training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createLinear\n",
      "creating: createLinear\n",
      "creating: createLogSoftMax\n",
      "creating: createClassNLLCriterion\n",
      "creating: createDLClassifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DLClassifier_7a1c9a347dd9"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create BigDL Model\n",
    "bigDLModel = Sequential().add(Linear(n_input, n_hidden_1)).add(Linear(n_hidden_1, n_classes)).add(LogSoftMax())\n",
    "classnll_criterion = ClassNLLCriterion()\n",
    "dlClassifier = DLClassifier(model=bigDLModel, criterion=classnll_criterion, feature_size=[n_input])\n",
    "dlClassifier.setLabelCol(\"label\").setMaxEpoch(training_epochs).setBatchSize(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "initial model training finished.\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "model = dlClassifier.fit(cc_data_training)\n",
    "print(\"\\ninitial model training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import DataFrame, SQLContext\n",
    "# predictionDF = DataFrame(model.transform(cc_data_test), SQLContext(sc))\n",
    "# predictionDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['balance',\n",
       " 'sex',\n",
       " 'education',\n",
       " 'marriage',\n",
       " 'age',\n",
       " 'label',\n",
       " 'assembled',\n",
       " 'features']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#columns\n",
    "cc_data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select some columns\n",
    "predictionDF=model.transform(cc_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---------+--------+---+-----+--------------------+--------------------+----------+\n",
      "|balance|sex|education|marriage|age|label|           assembled|            features|prediction|\n",
      "+-------+---+---------+--------+---+-----+--------------------+--------------------+----------+\n",
      "|  10000|  1|        1|       1| 38|  2.0|[10000.0,1.0,1.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        1|       2| 24|  2.0|[10000.0,1.0,1.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 21|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 22|  1.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 22|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 23|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 24|  1.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 24|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 26|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 27|  1.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 27|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        2|       2| 38|  2.0|[10000.0,1.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        3|       1| 47|  2.0|[10000.0,1.0,3.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        3|       2| 22|  1.0|[10000.0,1.0,3.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        3|       2| 22|  1.0|[10000.0,1.0,3.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        3|       2| 23|  1.0|[10000.0,1.0,3.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  1|        3|       2| 23|  1.0|[10000.0,1.0,3.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  2|        1|       2| 22|  1.0|[10000.0,2.0,1.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  2|        2|       1| 31|  1.0|[10000.0,2.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "|  10000|  2|        2|       2| 22|  1.0|[10000.0,2.0,2.0,...|[0.07718600707791...|       1.0|\n",
      "+-------+---+---------+--------+---+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#top 20 rows\n",
    "predictionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Area under precision-recall curve: = 1.0\n",
      "\n",
      "recall = 0.7753492193919475\n",
      "\n",
      "Precision = 0.6011664120117023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[balance: int, sex: int, education: int, marriage: int, age: int, label: double, assembled: vector, features: vector, prediction: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Evolution\n",
    "predictionDF.cache()\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "auPRC = evaluator.evaluate(predictionDF)\n",
    "print(\"\\nArea under precision-recall curve: = \" + str(auPRC))\n",
    "    \n",
    "recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictionDF)\n",
    "print(\"\\nrecall = \" + str(recall))\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictionDF)\n",
    "print(\"\\nPrecision = \" + str(precision))\n",
    "predictionDF.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Evolution\n",
    "y_pred = np.array(predictionDF.select('prediction').collect())\n",
    "y_true = np.array(predictionDF.select('label').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is 77.53%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prediction accuracy\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "print(\"The prediction accuracy is %.2f%%\"%(acc*100))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm.shape\n",
    "df_cm = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(df_cm, annot=True,fmt='d');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
